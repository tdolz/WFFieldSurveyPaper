---
title: "Field survey April 2020 GITHUB VERSION"
author: "tara"
date: "7/28/2020"
output:
  html_document: 
    highlight: tango
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, cache=TRUE)
```

## R Markdown

**GITHUB VERSION**

**COHORTS ONLY VERSION**

This is the cohort analysis section developed from CatchCurvesApril2020_VER4.Rmd  
We could try to compare mortality but think about this more. 

## Hypotheses ##
- cohorts in Shinnecock Bay during the first weeks post settlment (relative to that cohort) have different growth rates. - Lag the cohorts.  
- cohorts temporally have the same growth. (do not lag the cohorts)

**In this version we have used corrected catch +1 to calculate mortality slopes

**This markdown script includes**
- Cohort assignment including a comparison of mixture vs. normal model fits. 
- alter the plot mixEM function to fix the density bars. 
- function to do cohort assignment with mixtools
- A multi-histogram function that we are not using because it's broken
- another broken multi-histogram function with a custom distribution
- cohort assignment applied to all bays and years. 
- a comparison of cohorts assigned by our function to those assigned by age. 
-  mortality regression standardized to days since peak
- extract coefficients of regression
- facet plot fo cohort regression in Shinnecock
- shinnecock and mattituck mortality ANCOVAs, Tukey Test and LS means 
- extract mortality estimate for each cohort from catch curves
- barplots of mortality estimates
- cohort regressions, ANCOVAS, tests, plots for growth
- ggridges plot for cohorts
- extract regression coefficients for growth from each cohort
-  barplots of regression estimates for growth
-


```{r, include=FALSE}
#keeping everything in this folder
setwd("/Users//tdolan/Documents//R-Github//WFFieldSurveyPaper")

library("dplyr")
library("devtools")
library("tidyr")
library("moments")
library("lubridate")
library("ggplot2")
library('purrr')
library("agricolae")
library("lsmeans")
library("FSA")
library("chron")
library("cowplot")
```

Load the data & some extra data minging. 
```{r, include=FALSE}
#somedata3<-read.csv("somedata3_Mar-4-2020.csv", na.strings="", header=TRUE)
somedata3 <-read.csv("skcompiled4gams2.csv", na.strings="", header=TRUE)
somedata4 <-dplyr::select(somedata3,-rcpT,-cpue) %>% mutate(Date = as.Date(Date))

#mattituck data
some_mt_data <-read.csv("some_mt_data.csv", header=TRUE)
some_mt_data <-mutate(some_mt_data, Year=as.factor(Year), Date=as.Date(Date),catch1=cpW+1)%>%dplyr::rename(corrCatch=cpW)

#the CPUE is currently aggregated to tow, but has to be aggregated to week. 
byweek <- plyr::ddply(somedata4, Bay~Date~Year, summarize, avTemp = mean(Temp), avSAL=mean(SAL), avDO=mean(DO.mg.L), rawcatch=sum(NumCaught), corrCatch=sum(cpT),catch1=sum(cpT)+1, area=sum(area))

byweek <-mutate(byweek, Year=as.factor(Year))
byweek <-bind_rows(byweek,some_mt_data)

byweek <-dplyr::select(byweek, -Week) %>% mutate(cpue=catch1/area) %>% mutate(lncpue=log(cpue))%>% filter(!is.na(area)) 

#create day- year since common start date because it's not plotting them lined up. 
byweek <-separate(byweek, Date, c("year","m","d"), remove=FALSE) %>%unite(col="Day",m,d, sep="-") 
```

### create the alledays dataframe ###
adapted from the script "field graphs 9_10_19.R"
We are counting the peak as the PEAK and the slope as days since the peak, which is day 0. If you want to change this, alter this chunk. 
```{r, include=FALSE, fig.align='center', echo=FALSE}
#edays and week function
eday <- function(df){
  edays <-c()
  Week <-c()
for (i in 2:length(df$Date)){
  edays[i] <-df$Date[i]-df$Date[1]}
edays[1] <-0
Week <-rownames(df)
df <-cbind(df,edays,Week)
}


#days elapsed since peak function
ppp <- function(df){
past.peak <-c()
pp <-filter(df, !is.na(peak))
for (i in 2:length(pp$Date)){
  past.peak[i] <-pp$Date[i]-pp$Date[1]}
past.peak[1] <-0
pp <-cbind(pp, past.peak)
df <-full_join(df,pp)
}

byweek <-mutate(byweek, Date=as.Date(Date))
#honestly its probably easier to create separate dataframes for each. 
#Shinnecock 2016
Sh_16 <- filter(byweek, Bay=="Shinnecock", Year== "2016") %>% mutate(Date=as.Date(Date))
Sh_16 <-eday(Sh_16) %>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 8, Day,NA)) 
Sh_16 <-ppp(Sh_16)
#Shinnecock 2017
Sh_17 <- filter(byweek, Bay=="Shinnecock", Year== "2017") %>% mutate(Date=as.Date(Date))
Sh_17 <-eday(Sh_17)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 4, Day,NA))
Sh_17 <-ppp(Sh_17)
#Shinnecock 2011
Sh_11 <- filter(byweek, Bay=="Shinnecock", Year== "2011") %>% mutate(Date=as.Date(Date))
Sh_11 <-eday(Sh_11)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 6, Day,NA))
Sh_11 <-ppp(Sh_11)
#Shinnecock 2010
Sh_10 <- filter(byweek, Bay=="Shinnecock", Year== "2010") %>% mutate(Date=as.Date(Date))
Sh_10 <-eday(Sh_10)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 3, Day,NA))
Sh_10 <-ppp(Sh_10)
#Shinnecock all years
Sh_all <-bind_rows(Sh_16,Sh_17,Sh_10,Sh_11)

#Napeague 2016
N_16 <- filter(byweek, Bay=="Napeague", Year== "2016") %>% mutate(Date=as.Date(Date))
N_16 <-eday(N_16)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 5, Day,NA))
N_16 <-ppp(N_16)
#Napeague 2010
N_10 <- filter(byweek, Bay=="Napeague", Year== "2010") %>% mutate(Date=as.Date(Date))
N_10 <-eday(N_10)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 1,Day,NA))
N_10 <-ppp(N_10)
#Napeague all years
N_all <-bind_rows(N_16, N_10)

#Moriches 2016
M_16 <- filter(byweek, Bay=="Moriches", Year== "2016") %>% mutate(Date=as.Date(Date))
M_16 <-eday(M_16)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 5,Day,NA))
M_16 <-ppp(M_16)
#Moriches 2011
M_11 <- filter(byweek, Bay=="Moriches", Year== "2011") %>% mutate(Date=as.Date(Date))
M_11 <-eday(M_11)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 3, Day,NA))
M_11 <-ppp(M_11)
#Moriches 2010
M_10 <- filter(byweek, Bay=="Moriches", Year== "2010") %>% mutate(Date=as.Date(Date))
M_10 <-eday(M_10)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 2, Day,NA))
M_10 <-ppp(M_10)
#All Moriches
M_all <- bind_rows(M_16, M_11, M_10)

#Jamaica 2016
J_16 <- filter(byweek, Bay=="Jamaica", Year== "2016") %>% mutate(Date=as.Date(Date))
J_16 <-eday(J_16)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 1, Day,NA))
J_16 <-ppp(J_16)
#Jamaica 2011
J_11 <- filter(byweek, Bay=="Jamaica", Year== "2011") %>% mutate(Date=as.Date(Date))
J_11 <-eday(J_11) %>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >=2, Day,NA))
J_11 <-ppp(J_11)
#Jamaica 2010
J_10 <- filter(byweek, Bay=="Jamaica", Year== "2010") %>% mutate(Date=as.Date(Date))
J_10 <-eday(J_10)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >=1, Day,NA))
J_10 <-ppp(J_10)
#All Jamaica
J_all <-bind_rows(J_16,J_11,J_10)

#Cold Spring Pond 2010
C_10 <- filter(byweek, Bay=="Cold Spring Pond", Year== "2010") %>% mutate(Date=as.Date(Date))
C_10 <-eday(C_10) %>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week > 2, Day,NA))
C_10 <-ppp(C_10)

#Mattituck 2016
Mt_16 <- filter(byweek, Bay=="Mattituck", Year== "2016") %>% mutate(Date=as.Date(Date))
Mt_16 <-eday(Mt_16) %>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 5,Day,NA))
Mt_16 <-ppp(Mt_16)
#Mattituck 2015
Mt_15 <- filter(byweek, Bay=="Mattituck", Year== "2015") %>% mutate(Date=as.Date(Date))
Mt_15 <-eday(Mt_15)%>% mutate(Week=as.numeric(as.character(Week))) %>% mutate(peak=ifelse(Week >= 2, Day,NA))
Mt_15 <-ppp(Mt_15)
Mt_all <-bind_rows(Mt_16,Mt_15)

seventeen <-Sh_17%>% mutate(Date=as.Date(Date),Week=as.integer(Week))
sixteen <-bind_rows(J_16,M_16,Mt_16,N_16,Sh_16)%>% mutate(Date=as.Date(Date),Week=as.integer(Week))
fifteen <-Mt_15%>% mutate(Date=as.Date(Date),Week=as.integer(Week))
eleven <-bind_rows(J_11,M_11,Sh_11)%>% mutate(Date=as.Date(Date),Week=as.integer(Week))
ten <-bind_rows(C_10, J_10,M_10,N_10,Sh_10)%>% mutate(Date=as.Date(Date),Week=as.integer(Week))
alledays <-bind_rows(seventeen,sixteen,fifteen,eleven,ten)%>% unite(BayYear, Bay,Year, sep="_", remove=FALSE) %>%mutate(BayYear=as.factor(BayYear))
```


**Add Lyndie's to the rest of the data for later**
```{r, include=FALSE}
lylen<-read.csv("lyndie_lengths.csv", na.strings="", header=TRUE)
allmelt <-read.csv("allmeltdec2019.csv", na.strings="", header=TRUE)

#to make this work, there has to be a week column for 2010, 2011
lylen <-dplyr::select(lylen, - Tow) 

#catcorr function parameterized for shinnecock selectivity.  
catchcorrect <- function(ogl) #where OG length (ogl) is the original fish length
{
  cc <-(1+(1-1/(1+exp(-0.092437901714982*(ogl-51.4909728843284))))) #I couldn't get this to work by supplying custom arguments for g and lh, so just do it this way. 
  return(cc)
}
lylen <-mutate(lylen,catcorr=catchcorrect(lylen$TL), Year=as.factor(Year), Date=as.Date(Date)) %>% dplyr::rename(value=TL)

allmelt <-mutate(allmelt, value=as.numeric(as.character(value)), Year=as.factor(Year), Date=as.Date(Date), catcorr=as.numeric(as.character(catcorr)))  
allmelt <-bind_rows(allmelt,lylen)

#create a corrected catch vector.
allmelt <-mutate(allmelt, corTL= round(value*catcorr,0))
```

#Bay Histograms# 
For supplementary figures. 
```{r, fig.align='center', echo=FALSE}
allmelt <-separate(allmelt,Date,c("y","m","d"), remove=FALSE) %>%unite(Day, m, d, remove=TRUE) %>% dplyr::select(-y)
allmelt %>%
  filter(Bay=="Mattituck" & Year=="2015")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  xlab("length (mm)")+
  ggtitle("Mattituck 2015")+
  facet_wrap(~Day)+
  theme_bw()
#ggsave("mt15hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Mattituck" & Year=="2016")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
   xlab("length (mm)")+
  ggtitle("Mattituck 2016")+
  facet_wrap(~Day)+
  theme_bw()
#ggsave("mt16hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Jamaica" & Year=="2016")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Jamaica 2016")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("j16hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Jamaica" & Year=="2010")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Jamaica 2010")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("j10hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Jamaica" & Year=="2011")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Jamaica 2011")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("j11hist.png", path="/Users/tdolan/Documents/Proposal/Figures")


allmelt %>%
  filter(Bay=="Napeague" & Year=="2016")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Napeague 2016")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("n16hist.png", path="/Users/tdolan/Documents/Proposal/Figures") 

allmelt %>%
  filter(Bay=="Moriches" & Year=="2016")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Moriches 2016")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("m16hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Napeague" & Year=="2010")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Napeague 2010")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("n10hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Moriches" & Year=="2011")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Moriches 2011")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("m11hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Shinnecock" & Year=="2010")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Shinnecock 2010")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("sh10hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Shinnecock" & Year=="2011")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Shinnecock 2011")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("sh11hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Shinnecock" & Year=="2016")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Shinnecock 2016")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("sh16hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Shinnecock" & Year=="2017")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Shinnecock 2017")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("sh17hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Moriches" & Year=="2010")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Moriches 2010")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("m10hist.png", path="/Users/tdolan/Documents/Proposal/Figures")

allmelt %>%
  filter(Bay=="Cold Spring Pond" & Year=="2010")%>%
ggplot(aes(value))+
  geom_histogram(bins=20)+
  ggtitle("Cold Spring Pond 2010")+
  facet_wrap(~Day)+
  xlab("length (mm)")+
  theme_bw()
#ggsave("c10hist.png", path="/Users/tdolan/Documents/Proposal/Figures")
#dev.off()
```
Many of these bays look like they could have cohorts, but we didn't sample enough to detect them. For example Jamaica 2016 definitely has cohorts, but it's liek 2 fish - or else there are serious growth differences between individuals, which is very possible. 
I don't see how we could possibly split the Mattituck 2015 data into cohorts effectively. Could maybe split the 2016 data, but what's the point? You need to go back to Catherine's length-cohort assignments by age. but this was very difficult last time, to match up her data to this data because she only aged a small subset of individuals. So we'd have to get a length at age relationship from her data and then use that to classify individuals. That could be a way to do it. i don't think that's much better than doing it this way though. For now, let's just classify 2016 by cohort. 

**Alter the plot.mixEM function so that you can fix the density bars! **
```{r, echo=FALSE, include=FALSE}
plot.mixEMcustom <-function (x, whichplots = 1, loglik = 1 %in% whichplots, density = 2 %in% 
    whichplots, xlab1 = "Iteration", ylab1 = "Log-Likelihood", 
    main1 = "Observed Data Log-Likelihood", col1 = 1, lwd1 = 2, 
    xlab2 = NULL, ylab2 = NULL, main2 = NULL, col2 = NULL, lwd2 = 2, 
    alpha = 0.05, marginal = FALSE, ...) 
{
    def.par <- par(ask = (loglik + density > 1), "mar")
    mix.object <- x
    if (!inherits(mix.object, "mixEM")) 
        stop("Use only with \"mixEM\" objects!")
    if (loglik) {
        plot(mix.object$all.loglik, xlab = xlab1, ylab = ylab1, 
            main = main1, type = "l", lwd = lwd1, col = col1, 
            ...)
    }
        if (mix.object$ft == "normalmixEM") {
            k <- ncol(mix.object$posterior)
            x <- sort(mix.object$x)
            a <- hist(x, plot = FALSE)
            maxy <- max(max(a$density), 0.3989 * mix.object$lambda/mix.object$sigma)
            if (is.null(main2)) {
                main2 <- "Density Curves"
            }
            if (is.null(xlab2)) {
                xlab2 <- "Data"
            }
            if (is.null(col2)) {
                col2 <- 2:(k + 1)
            }
            hist(x, prob = TRUE, main = main2, xlab = xlab2, 
               # ylim = c(0, maxy), ################################## get rid of ylim. 
                ...)
            if (length(mix.object$mu) == 1) {
                arbvar <- TRUE
                mix.object$sigma <- mix.object$scale * mix.object$sigma
                arbmean <- FALSE
            }
            if (length(mix.object$mu) == k && length(mix.object$sigma) == 
                1) {
                arbmean <- TRUE
                arbvar <- FALSE
            }
            if (length(mix.object$sigma) == k && length(mix.object$mu) == 
                k) {
                arbmean <- TRUE
                arbvar <- TRUE
            }
            for (i in 1:k) {
                lines(x, mix.object$lambda[i] * dnorm(x, mean = mix.object$mu[i * 
                  arbmean + (1 - arbmean)], sd = mix.object$sigma[i * 
                  arbvar + (1 - arbvar)]), col = col2[i], lwd = lwd2)
            }
        }
       
        
        if (mix.object$ft == "regmixEM") {
            if (ncol(mix.object$x) != 2) {
                stop("The predictors must have 2 columns!")
            }
            post <- apply(mix.object$posterior, 1, which.max)
            k <- ncol(mix.object$posterior)
            x <- mix.object$x[, 2]
            y <- mix.object$y
            n <- length(y)
            if (is.null(main2)) {
                main2 <- "Most Probable Component Membership"
            }
            if (is.null(xlab2)) {
                xlab2 <- "Predictor"
            }
            if (is.null(ylab2)) {
                ylab2 <- "Response"
            }
            if (is.null(col2)) {
                col2 <- 2:(k + 1)
            }
            plot(x, y, main = main2, xlab = xlab2, ylab = ylab2, 
                type = "n", ...)
            a = cbind(mix.object$x[, 2], mix.object$y, post)
            for (i in 1:k) {
                xy = subset(cbind(a, mix.object$posterior[, i]), 
                  a[, 3] == i)[, -3]
                xy = matrix(xy, ncol = 3)
                points(xy[, 1], xy[, 2], col = col2[i])
                if (is.matrix(mix.object$beta) == FALSE) {
                  abline(coef = mix.object$beta)
                  beta = matrix(mix.object$beta, ncol = k, nrow = 2)
                }
                else {
                  abline(coef = mix.object$beta[, i], col = col2[i])
                  beta = mix.object$beta
                }
                out = lm(y ~ x, weights = mix.object$posterior[, 
                  i])
                fit = beta[1, i] + beta[2, i] * x
                out.aov = anova(out)
                MSE = out.aov$Mean[2]
                xy.f = cbind(x, y, fit)
                xy.sort = xy.f[order(xy.f[, 1]), ]
                x.new = seq(from = min(x), to = max(x), length.out = 100)
                y.new = beta[1, i] + beta[2, i] * x.new
                s.h <- sqrt(MSE * (1/n + (x.new - mean(xy.sort[, 
                  1]))^2/var(xy.sort[, 1])/(n - 1)))
                for (j in 1:length(alpha)) {
                  W = sqrt(qf(1 - alpha[j], 2, n - 2))
                  upper = y.new + W * s.h
                  lower = y.new - W * s.h
                  lines(x.new, upper, col = (i + 1))
                  lines(x.new, lower, col = (i + 1))
                }
            }
        }
        if (mix.object$ft == "expRMM_EM") {
            plotexpRMM(mix.object, ...)
        }
        if (mix.object$ft == "weibullRMM_SEM") {
            plotweibullRMM(mix.object, ...)
        }
    }
    #par(def.par)
```

**Function to do cohort assignment with mixtools**
remember to change the breaks option to alter the histogram as needed. 
```{r, include=FALSE, echo=FALSE}
#install.packages("mixtools")
library("mixtools")
library("fitdistrplus")

#AIC from log likelihood
newAIC <-function(x,p) {2*p -2*x}

#function to visually inspect the data
vis_wk <- function(Values) {
  
  #brks = length(Values)/10
  #hist(Values, breaks=brks, freq=FALSE)
  hist(Values, freq=TRUE, xlab="length (mm)", main="")
  
  #these are the mean and variance of the COMBINED distribution
  avg <- mean(Values)
  vr <- var(Values)
  sd <-sd(Values)
  min <-min(Values)
  max <-max(Values)
  parms <-c(avg,vr,sd,min,max)
  names(parms) <-c("Mean","Var","SD","Min","Max")
  return(parms)
}
#function to use normalmixEM to classify data
chrt_wk <-function(Values,avg1,avg2,sig1,sig2,lam1,lam2) {
  mix <- normalmixEM(Values,mu=c(avg1,avg2),sigma=c(sig1,sig2),lambda=c(lam1,lam2)) #DOES NOT assume mixtures have equal variances.
  aicmix <-newAIC(mix$loglik,6)
  brks = length(unique(Values))/2
  #brks = length(unique(Values))
  #plot(mix,which=2,main2="",ylab2="density",xlab2="length (mm)",breaks=brks,
  plot.mixEMcustom(mix,which=2,main2="",ylab2="density",xlab2="length (mm)",breaks=brks,  #toggle between custom plot function and the 
  )
  lines(density(Values),lty=2,lwd=2)
  mix.params<-as.numeric(c(mix$mu[1], mix$mu[2], mix$lambda[1], mix$lambda[2],mix$sigma[1],mix$sigma[2],mix$loglik, aicmix))
  names(mix.params) <-c("mu1","mu2","lambda1","lambda2","sigma1","sigma2","loglik","AIC")
  return(mix.params)
 # return(c("mu=",mix$mu,"lambda=", mix$lambda,"sigma=",mix$sigma,"loglik=",mix$loglik,"AIC=",aicmix))
}
  #Plot the norm dist
  normfit <-function(Values){
   # tryCatch(exp= {
  FIT <-fitdist(Values, "norm")
  plot(FIT)
  norm.params<-c(FIT$estimate[1],FIT$sd[1],FIT$estimate[2],FIT$sd[2],FIT$loglik,FIT$aic, FIT$n)
  names(norm.params)<-c("mean","std.err.mean","sd","std.err.sd","loglik","AIC","n")
  return(norm.params)
  #  },error=function(e){})
  }
```


**Split 2016**
```{r}
split_df <- allmelt %>%
  filter(Bay == "Shinnecock")%>% filter(Year=="2016")%>%
  split(.$Week) 

#normfits
normfits <- allmelt %>%
  filter(Bay == "Shinnecock")%>% filter(Year=="2016")%>%
  split(.$Week) %>% discard(function(df) length(df$value)<3)%>% purrr::map_dfr(function(df) normfit(df$value))
normfits <- t(normfits)
colnames(normfits) <- c("mean","std.err.mean","sd","std.err.sd","loglik","AIC","n")

#visparams
vis_params <-allmelt %>%
  filter(Bay == "Shinnecock")%>% filter(Year=="2016")%>%
  split(.$Week) %>% purrr::map_dfr(function(df) vis_wk(df$value))
vis_params <- t(vis_params)
colnames(vis_params) <- c("mean_obs","var_obs","sd_obs","min_obs","max_obs")

#(Values,avg1,avg2,sig1,sig2,lam1,lam2,brks)

#week1
wk1.16 <- as.data.frame(split_df$`1`) %>% mutate(co= 1)
#week2
chrt_wk(split_df$`2`$value,26,36,2,2,0.4344683,	0.5655317)#5
wk2.16 <- as.data.frame(split_df$`2`) %>% mutate(co= 1) 
#week3
chrt_wk(split_df$`3`$value,30,50,2,2,0.8659636,	0.1340364)#15
wk3.16 <- as.data.frame(split_df$`3`) %>% mutate(co= 1) #NO SPLIT
#wk3.16 <- as.data.frame(split_df$`3`) %>% mutate(co=ifelse(value <= 47,2,1))#SPLIT
#week4
chrt_wk(split_df$`4`$value,30,50,3,4,0.08400393,	0.91599607) #12
#wk4.16 <- as.data.frame(split_df$`4`) %>% mutate(co= ifelse(value <= 30,2,1)) # SPLIT
wk4.16 <- as.data.frame(split_df$`4`) %>% mutate(co= 1) #NO SPLIT DESPITE AIC
#week5
chrt_wk(split_df$`5`$value,25,55,5,5,0.3586201,	0.6413799)#10
wk5.16 <- as.data.frame(split_df$`5`) %>% mutate(co= ifelse(value <= 36,2,1))
#week6
chrt_wk(split_df$`6`$value,30,55,5,5,0.2368258,	0.7631742)#12
wk6.16 <- as.data.frame(split_df$`6`) %>% mutate(co= ifelse(value <= 40,2,1))
#week7
chrt_wk(split_df$`7`$value,40,65,5,5,0.4532022,	0.5467978)#12
wk7.16 <- as.data.frame(split_df$`7`) %>% mutate(co= ifelse(value <= 53,2,1))
#week8
chrt_wk(split_df$`8`$value,45,70,5,5,0.7087908,	0.2912092)#12
wk8.16 <- as.data.frame(split_df$`8`) %>% mutate(co= ifelse(value <= 55,2,1)) #actually crossed at 51
#week9
chrt_wk(split_df$`9`$value,45,80,12,12,0.6727435,	0.3272565)#15
wk9.16 <- as.data.frame(split_df$`9`) %>% mutate(co= ifelse(value <= 63,2,1)) 
#week10
chrt_wk(split_df$`10`$value,45,80,5,5,0.6504205,	0.3495795)#5
wk10.16 <- as.data.frame(split_df$`10`) %>% mutate(co= ifelse(value <= 65,2,1))
#week11
chrt_wk(split_df$`11`$value,45,85,16,4,0.8192995,	0.1807005)#5
wk11.16 <- as.data.frame(split_df$`11`) %>% mutate(co= ifelse(value <= 80,2,1)) #check this. depending on how you parameterize it could go either way. 
#week12
chrt_wk(split_df$`12`$value,50,80,8,3,0.892547,	0.107453)#15
wk12.16 <- as.data.frame(split_df$`12`) %>% mutate(co= ifelse(value <= 82,2,1)) #check this
#week13
chrt_wk(split_df$`13`$value,50,100,6,2,0.2701039,	0.7298961)#5
wk13.16 <- as.data.frame(split_df$`13`) %>% mutate(co= 2)
#week14
chrt_wk(split_df$`14`$value,65,85,10,1,0.90238756,	0.09761244)#10
#wk14.16 <- as.data.frame(split_df$`14`) %>% mutate(co= ifelse(value <= 90,2,1))
wk14.16 <- as.data.frame(split_df$`14`) %>% mutate(co= 2)
#week15
#chrt_wk(split_df$`15`$value,65,85,10,1,2)
hist(split_df$`15`$value)
wk15.16 <- as.data.frame(split_df$`15`) %>% mutate(co= 2)

shinco2016 <-bind_rows(wk1.16, wk2.16, wk3.16, wk4.16, wk5.16, wk6.16, wk7.16, wk8.16, wk9.16, wk10.16, wk11.16, wk12.16, wk13.16, wk14.16, wk15.16)

shinco2016 %>%
ggplot(aes(value)) + 
    geom_histogram(data = subset(shinco2016, co==1), fill = "#a6bddb", alpha = 0.8) + 
    geom_histogram(data = subset(shinco2016, co==2), fill = "#1c9099", alpha = 0.8) +
    facet_wrap(~Date, nrow=10)+
    ylab("count")+xlab("length (mm)")+
    theme_classic()

#plot(mix,which=2, breaks=5,xlab2="length (mm)",main2="")
```

**Split 2017**
```{r}
split_df <- allmelt %>%
  filter(Bay == "Shinnecock")%>% filter(Year=="2017")%>%
  split(.$Week) 

#normfits
normfits <- allmelt %>%
  filter(Bay == "Shinnecock")%>% filter(Year=="2017")%>%
  split(.$Week) %>% discard(function(df) length(df$value)<3)%>% purrr::map_dfr(function(df) normfit(df$value))
normfits <- t(normfits)
colnames(normfits) <- c("mean","std.err.mean","sd","std.err.sd","loglik","AIC","n")

#visparams
vis_params <-allmelt %>%
  filter(Bay == "Shinnecock")%>% filter(Year=="2017")%>%
  split(.$Week) %>% purrr::map_dfr(function(df) vis_wk(df$value))
vis_params <- t(vis_params)
colnames(vis_params) <- c("mean_obs","var_obs","sd_obs","min_obs","max_obs")

#week1
chrt_wk(split_df$`1`$value,26,50,1,5,0.1321875,	0.8678125)#10
wk1.17 <- as.data.frame(split_df$`1`) %>% mutate(co= ifelse(value <= 33,2,1))
#mix1 <-normalmixEM(split_df$`1`$value,mu=c(26,50),sig=c(1,5),lambda=c(0.1321875,	0.8678125))
#plot.mixEM(mix1,which=2,xlab2="length (mm)", main2="")
#week2
chrt_wk(split_df$`2`$value,25,55,2,2,0.4657784,	0.5342216)#20
wk2.17 <- as.data.frame(split_df$`2`) %>% mutate(co= ifelse(value <= 35,2,1)) 
#week3
chrt_wk(split_df$`3`$value,30,60,5,5,0.7445938,	0.2554062)#30
wk3.17 <- as.data.frame(split_df$`3`) %>% mutate(co= ifelse(value <= 40,2,1)) 
#week4
chrt_wk(split_df$`4`$value,35,70,5,5,0.6256111,	0.3743889)#50
wk4.17 <- as.data.frame(split_df$`4`) %>% mutate(co= ifelse(value <= 47,2,1))
#week5
chrt_wk(split_df$`5`$value,45,80,5,5,0.3670756,	0.6329244)#20
wk5.17 <- as.data.frame(split_df$`5`) %>% mutate(co= ifelse(value <= 51,2,1))
#week6 
chrt_wk(split_df$`6`$value,45,85,3,5,0.7773542,	0.2226458)#15
wk6.17 <- as.data.frame(split_df$`6`) %>% mutate(co= ifelse(value <= 68,2,1))
#week7
chrt_wk(split_df$`7`$value,50,73,16,1,0.95307954,	0.04692046)#50
wk7.17 <- as.data.frame(split_df$`7`) %>% mutate(co= ifelse(value <= 72,2,1))
#week8
chrt_wk(split_df$`8`$value,50,95,5,3,0.94047457,	0.05952543)#25
wk8.17 <- as.data.frame(split_df$`8`) %>% mutate(co= ifelse(value <= 85,2,1))
#week9
chrt_wk(split_df$`9`$value,45,90,12,2,0.331143,	0.668857)#10
wk9.17 <- as.data.frame(split_df$`9`) %>% mutate(co= ifelse(value <= 75,2,1))
#week10
chrt_wk(split_df$`10`$value,45,80,2,2,0.5,0.5) #no split
wk10.17 <- as.data.frame(split_df$`10`) %>% mutate(co= 2)
#week11
chrt_wk(split_df$`11`$value,50,100,2,12,0.2230176,	0.7769824)#10
#wk11.17 <- as.data.frame(split_df$`11`) %>% mutate(co= ifelse(value <= 90,2,1)) 
wk11.17 <- as.data.frame(split_df$`11`) %>% mutate(co= 2) 
#week13
chrt_wk(split_df$`13`$value,50,100,2,12,0.2230176,	0.7769824)
wk13.17 <-as.data.frame(split_df$`13`) %>% mutate(co=2)


shinco2017 <-bind_rows(wk1.17, wk2.17, wk3.17, wk4.17, wk5.17, wk6.17, wk7.17, wk8.17, wk9.17, wk10.17, wk11.17, wk13.17)

shinco2017 %>%
ggplot(aes(value)) + 
    geom_histogram(data = subset(shinco2017, co==1), fill = "#a6bddb", alpha = 0.8) + 
    geom_histogram(data = subset(shinco2017, co==2), fill = "#1c9099", alpha = 0.8) +
    facet_wrap(~Date, nrow=10)+
    ylab("count")+xlab("length (mm)")+
    theme_classic()

```

#now Mattituck Classification#
**Mattituck 2016 cohort classification**
```{r, include=FALSE}
split_df <- allmelt %>%
  filter(Bay == "Mattituck")%>% filter(Year=="2016")%>%
  split(.$Week) 

#normfits
normfits <- allmelt %>%
  filter(Bay == "Mattituck")%>% filter(Year=="2016")%>%
  split(.$Week) %>% discard(function(df) length(df$value)<3) %>% purrr::map_dfr(function(df) normfit(df$value))
normfits <- t(normfits)
colnames(normfits) <- c("mean","std.err.mean","sd","std.err.sd","loglik","AIC","n")

#visparams
vis_params <-allmelt %>%
  filter(Bay == "Mattituck")%>% filter(Year=="2016")%>%
  split(.$Week) %>% purrr::map_dfr(function(df) vis_wk(df$value))
vis_params <- t(vis_params)
colnames(vis_params) <- c("mean_obs","var_obs","sd_obs","min_obs","max_obs")

#week3
chrt_wk(split_df$`3`$value,35,50,2,1,0.7313829,	0.2686171)#10
wk3.11 <- as.data.frame(split_df$`3`) %>% mutate(co= 1) #NO SPLIT - based on AIC
#week5
chrt_wk(split_df$`5`$value,35,65,12,12,0.815753,	0.184247)#10
#mix5 <-normalmixEM(split_df$`5`$value,mu=c(35,65),sig=c(12,12),lambda=c(0.815753,	0.184247))
#rerun
#plot(mix5, which=2, xlab2="length (mm)", main2="",breaks=10)
#wk5.11 <- as.data.frame(split_df$`5`) %>% mutate(co= ifelse(value <= 49,2,1)) #SPLIT
wk5.11 <- as.data.frame(split_df$`5`) %>% mutate(co= 1) #NO SPLIT - based on visuals?  
#week7
chrt_wk(split_df$`7`$value,30,70,10,10,0.452067,	0.547933)#10
#rerun
#mix7 <-normalmixEM(split_df$`7`$value,mu=c(30,70),sig=c(10,10),lambda=c(0.452067,	0.547933))
#plot(mix7, which=2, xlab2="length (mm)", main2="",breaks=10)
wk7.11 <- as.data.frame(split_df$`7`) %>% mutate(co= ifelse(value <= 49,2,1)) 
#week9
chrt_wk(split_df$`9`$value,45,85,10,5,0.4534408,	0.5465592)#10
#rerun 
#mix9 <-normalmixEM(split_df$`9`$value,mu=c(45,85),sig=c(10,5),lambda=c(0.4534408,	0.5465592))
#plot(mix9, which=2, xlab2="length (mm)", main2="",breaks=10)
wk9.11 <- as.data.frame(split_df$`9`) %>% mutate(co= ifelse(value <= 52,2,1))
#week11
chrt_wk(split_df$`11`$value,45,85,10,5,0.4444444,	0.5555556)#10
#rerun 
#mix11 <-normalmixEM(split_df$`11`$value,mu=c(45,85),sig=c(10,5),lambda=c(0.4444444,	0.5555556))
#plot(mix11, which=2, xlab2="length (mm)", main2="",breaks=9)
wk11.11 <- as.data.frame(split_df$`11`) %>% mutate(co= ifelse(value <= 63,2,1)) 
#week13
chrt_wk(split_df$`13`$value,60,100,10,5,0.3333327,	0.6666673)#10
wk13.11 <- as.data.frame(split_df$`13`) %>% mutate(co= ifelse(value <= 75,2,1))
#week15
#chrt_wk(split_df$`15`$value,70,110,0.5)
wk15.11 <- as.data.frame(split_df$`15`) %>% mutate(co= 2) #NO SPLIT

Mtco2016 <-bind_rows(wk3.11,wk5.11,wk7.11,wk9.11,wk11.11,wk13.11,wk15.11)

Mtco2016 %>%
ggplot(aes(value)) + 
    geom_histogram(data = subset(Mtco2016, co==1), fill = "#a6bddb", alpha = 0.8) + 
    geom_histogram(data = subset(Mtco2016, co==2), fill = "#1c9099", alpha = 0.8) +
    facet_wrap(~Date, nrow=5)+
    ylab("count")+xlab("length (mm)")+
    theme_classic()

```



**Checking Matt's Fish**
I did this mostly in excel in a file called "mattyage3.csv" because i was too tired to datawrangle it in r. 
NEED TO REDO THIS ONCE COHORTS ARE RECLASSIFIED
```{r}
library("chron")
mattyage <-read.csv(file="mattyage4.csv", na.strings="", header=TRUE)
mattyage <-mutate(mattyage, bday=as.Date(bday), cohort=as.factor(cohort)) 

mattyage %>%
  filter(location=="Mattituck")%>%
#ggplot(aes(x=bday, y=catch.date,group=cohort))+
  ggplot(aes(x=bday, y=cohort,group=cohort))+
  geom_point(aes(color=cohort,size=TL))+
  scale_x_date(date_breaks="2 weeks")+
   xlab("settlement date")+
  #ggtitle("Mattituck 2016")+
  facet_grid(~Year, scales="free")+
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.background = element_rect(fill = 'white', colour = 'light gray'),
        panel.grid.major = element_line(colour = "white"))
```
It looks as if I'm not so good at cataloging the cohorts.
But it also looks like we really don't have enough aged fish to separate these manually. 

**Standardize the regression to days since peak for all years in Shinnecock**
REMEMBER CPUE is catch +1/area
We have edited this to not include lyndie's data because we are not doing cohort analysis those years. 
but a version that includes can be fount in CatchCurvesApril2020_VER4.Rmd.
```{r, include=FALSE}
#days elapsed since peak function - this should already exist from before. 
ppp <- function(df){
past.peak <-c()
pp <-filter(df, !is.na(peak))
for (i in 2:length(pp$Date)){
  past.peak[i] <-pp$Date[i]-pp$Date[1]}
past.peak[1] <-0
pp <-cbind(pp, past.peak)
df <-full_join(df,pp)
}

#Shinnecock Data
all.sco <-bind_rows(shinco2016,shinco2017)
splitco <-all.sco%>%
  split(list(.$Year,.$co)) 

shi16_e <-plyr::ddply(splitco$`2016.1`, ~Week, summarize, cpW =sum(catcorr),avl= weighted.mean(value,catcorr),minl = min(value),maxl = max(value), sdl=sd(value),Date=max(Date)) %>%mutate(Week=as.numeric(as.character(Week)), co="1", catch1=cpW+1) %>% mutate(starred=ifelse(Week %in% c(5,6,7,8,9),"star","nostar"))
shi16_e <- mutate(shi16_e, peak=ifelse(Week >= 3, Date,NA)) %>% ppp()

shi16_l <-plyr::ddply(splitco$`2016.2`, ~Week, summarize, cpW =sum(catcorr),avl= weighted.mean(value,catcorr),minl = min(value),maxl = max(value), sdl=sd(value),Date=max(Date)) %>%mutate(Week=as.numeric(as.character(Week)), co="2", catch1=cpW+1)%>% mutate(starred=ifelse(Week %in% c(5,6,7,8,9),"star","nostar"))
shi16_l <- mutate(shi16_l, peak=ifelse(Week >= 9, Date,NA)) %>% ppp()

shi17_e <-plyr::ddply(splitco$`2017.1`, ~Week, summarize, cpW =sum(catcorr),avl= weighted.mean(value,catcorr),minl = min(value),maxl = max(value), sdl=sd(value),Date=max(Date)) %>%mutate(Week=as.numeric(as.character(Week)), co="1", catch1=cpW+1) %>% mutate(starred=ifelse(Week %in% c(1,2,3,4,5,6),"star","nostar"))
shi17_e <- mutate(shi17_e, peak=ifelse(Week >= 4, Date,NA)) %>% ppp()

shi17_l <-plyr::ddply(splitco$`2017.2`, ~Week, summarize, cpW =sum(catcorr),avl= weighted.mean(value,catcorr),minl = min(value),maxl = max(value), sdl=sd(value),Date=max(Date)) %>%mutate(Week=as.numeric(as.character(Week)), co="2", catch1=cpW+1)%>% mutate(starred=ifelse(Week %in% c(1,2,3,4,5,6),"star","nostar"))
shi17_l <- mutate(shi17_l, peak=ifelse(Week >= 4, Date,NA)) %>% ppp() #keep changing my mind about what peak to use. it also could be 4

#cool, ok but we forgot about effort, so we also have to attach the lyndie data to effort data. 
Sh16_eff <- dplyr::select(Sh_16, area, Date, Bay, Year)
Sh17_eff <- dplyr::select(Sh_17, area, Date, Bay, Year)
  
#calculate lncpue the correct way, where one fish is added to the selectivity corrected raw catch. 
shi16_e <-left_join(shi16_e, Sh16_eff, by=c("Date")) %>% mutate(cpue=catch1/area) %>%mutate(lncpue=log(cpue))
shi16_l <-left_join(shi16_l, Sh16_eff, by=c("Date")) %>% mutate(cpue=catch1/area) %>%mutate(lncpue=log(cpue))
shi17_e <-left_join(shi17_e, Sh17_eff, by=c("Date")) %>% mutate(cpue=catch1/area) %>%mutate(lncpue=log(cpue))
shi17_l <-left_join(shi17_l, Sh17_eff, by=c("Date")) %>% mutate(cpue=catch1/area) %>%mutate(lncpue=log(cpue))

#annoyingly, we must do some type conversion first, or drop variables that are causing type conversion issues. 
shin17n17 <-bind_rows(shi16_e,shi16_l,shi17_e,shi17_l)
all.co.shin <-dplyr::select(shin17n17,-peak) %>% separate(Date, c("Year","m","d"), remove=FALSE) %>%unite(Day,m,d,sep="-") %>% mutate(Bay="Shinnecock",Week=as.factor(Week))
head(all.co.shin)
all.co.shin <-filter(all.co.shin, !is.na(catch1))
```

**Meld Mattituck 2016 Data with effort** 
```{r, include=FALSE}
#summarize catch per week by cohort in Mtco2016
Mtcocatch16 <-plyr::ddply(Mtco2016, Week~co~Date,summarize, cpW=sum(catcorr)) %>% mutate(starred=ifelse(Week %in% c(7,9,11,13),"star","nostar"))
#pull 2016 effort from some mt data by date
Mteffco16 <-dplyr::select(some_mt_data, Date,Year, area) %>% filter(Year==2016)%>%mutate(Date=as.Date(Date))
# join them 
Mtcocatch16 <-left_join(Mtcocatch16, Mteffco16,by=c("Date")) %>%mutate(cpue=cpW/area)%>%mutate(lncpue=log(cpue))%>%separate(Date, c("year","m","d"), remove=FALSE) %>%unite(col="Day",m,d, sep="-") 
#split
splitmt <-Mtcocatch16 %>%
  split(.$co)
#calculate peaks
mt16_e <-mutate(splitmt$`1`, peak=ifelse(Week >= 3, Day,NA))
mt16_e <-ppp(mt16_e)
mt16_l <-mutate(splitmt$`2`, peak=ifelse(Week >= 9, Day,NA))
mt16_l <-ppp(mt16_l)
#now rebind
Mtcocatch <-bind_rows(mt16_e,mt16_l)
```
The reason why there is missing data is because each date should have at least one line, but *up to two lines* because there are possibly two cohorts. 
If there is effort, but no fish *from that cohort* for that day, it's zero data, and should be eliminated?? or is it a one? I think that, for consistency,
in this case it should get a one, but doing so creates problems in the catch curve later on. The facet plot. 


**Extract coefficients from the regression for each co and year**
Linear regression for each bay and year
Shinnecock
```{r}
#Shinnecock
all.co.shin.naomit <-filter(all.co.shin,!is.na(past.peak))%>%unite(YearCo, Year,co, remove=FALSE)

#function to extract model p.value
lmp <- function (modelobject) {
    if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
    f <- summary(modelobject)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)}

#split dataframe into a list of multiple data frames
models <-all.co.shin.naomit %>% base::split(.$YearCo)%>%
 purrr::map(function(df) lm(lncpue~past.peak,data = df))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
shi.ests<-as.data.frame(tibble::rownames_to_column(ests, var ="SplitVar"))
```

**Extract coefficients from the regression for each co and year**
Linear regression for each bay and year
Mattituck
```{r}
Mtcocatch.naomit <-filter(Mtcocatch,!is.na(past.peak)) %>%unite(YearCo, year,co, remove=FALSE)
#split dataframe into a list of multiple data frames
models <-Mtcocatch.naomit %>% base::split(.$YearCo)%>%
 purrr::map(function(df) lm(lncpue~past.peak,data = df))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
mt.ests<-as.data.frame(tibble::rownames_to_column(ests, var ="SplitVar"))
```

**Facet plot for cohort regression in Shinnecock**
I think these facet plots are nicer if we can clean them up and get the R squared to display properly. Also the x axis ticks will have to be tweaked. 
```{r, fig.align='center', echo=FALSE}

#library("wesanderson")
cohortcolors <-c("#a6bddb","#1c9099")

#facet plot. 
my.formula <- y ~ x
all.co.shin %>% 
  #mutate(Date = as.Date(Date))%>%
  ggplot(aes(y=lncpue,x=Date))+
  geom_point(aes(group=co,y=lncpue,x=Day,color=co, fill=co))+
  #scale_fill_manual(values=wes_palette(n=2, name="GrandBudapest2"))+
  scale_fill_manual(values=cohortcolors)+ scale_color_manual(values=cohortcolors)+
  scale_x_discrete(breaks=c("06-01","07-01","08-04"),labels=c("June","July","August"))+
  #scale_x_date(labels = date_format("%m/%d"),breaks = date_breaks("2 weeks"))+
   #scale_x_date(date_breaks="2 weeks")+
  geom_smooth(aes(group=co,y=lncpue,x=Day,color=co, fill=co), data=filter(all.co.shin, !is.na(past.peak)), method="lm", formula= my.formula, se=TRUE, fullrange=FALSE)+
  #stat_poly_eq(formula = my.formula, 
       #     aes(group=co,y=lncpue,x=past.peak,label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
        #    parse = TRUE) +     
  ylab("LN CPUE")+
  #ylim()+
  facet_grid(~Year)+
  #facet_grid(co~Year)+
  #theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = 'white', colour = 'light gray'),
        #panel.grid.major = element_line(colour = "white"))
  theme_cowplot()
#ggsave("cohortmortshin.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()
```
*I am not happy with the peak week assignment for 2011. I am not happy with how these cohorts were assigned.* 
we need to think about this more. I mean they look kind of different, but that could be because 2011 is messed up. The ecological argument for them being different is bet hedging. The ecological argument for them being the same is constant YOY mortality due to constant predation. Update: it looks a little better when the peak for early cohort 2011 is week 3 (first data point) and the week in the late cohort is week 5 (second data point)

#Is mortality different between cohorts in Mattituck?#
**Plot**
```{r, fig.align='center', echo=FALSE}
Mtcocatch <- mutate(Mtcocatch, co=as.factor(co))
my.formula <- y ~ x
Mtcocatch%>%
ggplot(aes(y=lncpue,x=Day))+
  geom_point(aes(group=co,y=lncpue,x=Day,color=co, fill=co))+
  scale_fill_manual(values=cohortcolors)+ scale_color_manual(values=cohortcolors)+
  scale_x_discrete()+
  geom_smooth(aes(group=co,y=lncpue,x=Day,color=co, fill=co), data=filter(Mtcocatch, !is.na(past.peak)), method="lm", formula= my.formula, se=TRUE, fullrange=FALSE)+
  #stat_poly_eq(formula = my.formula, 
           # aes(group=co,y=lncpue,x=past.peak,label = paste(..eq.label.., ..rr.label.., sep = "~~~")), 
            #parse = TRUE) +     
  ylab("LN CPUE")+
  #ggtitle("Mattituck Cohorts")+
  facet_wrap(~Year)+
   theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = 'white', colour = 'light gray'),panel.grid.major = element_line(colour = "white"))
#ggsave("mtcohortmort.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()

```

**Plot them together**
```{r}
allslopes <-dplyr::select(all.co.shin,Day, Year, co, lncpue, Bay, past.peak)
mtco <-dplyr::select(Mtcocatch, Day, Year,lncpue,co,past.peak) %>% mutate (Bay="Mattituck")
allslopes <-bind_rows(allslopes, mtco)
allslopes <-unite(allslopes, BayYear, Bay, Year,sep=" ", remove=FALSE)

#facet plot. 
my.formula <- y ~ x
allslopes %>% 
ggplot(aes(y=lncpue,x=Day))+
  geom_point(aes(group=co,y=lncpue,x=Day,color=co, fill=co))+
  scale_fill_manual(values=cohortcolors)+ scale_color_manual(values=cohortcolors)+
  #scale_x_discrete()+
  scale_x_discrete(breaks=c("06-02","07-11","08-08","06-01","07-01","08-04","06-02","07-05","08-02"),
                   labels=c("June","July","August","June","July","August","June","July","August"))+
  #geom_smooth(aes(group=co,y=lncpue,x=Day,color=co, fill=co), data=filter(allslopes, !is.na(past.peak)), method="lm", formula= my.formula, se=TRUE, fullrange=FALSE)+
  ylab("LN CPUE")+
  facet_wrap(~BayYear, scales="free_x")+
  #theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = 'white', colour = 'light gray'),
       # panel.grid.major = element_line(colour = "white"))
  theme_cowplot()
  #theme_classic()
#ggsave("cohortslopesALL.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()

```



If it turns out that mortality is worse for the later cohort, it's good to have the days, and then with growth we have size at age. *There's something here about size by a certain date that we have to tease out of these datasets somehow.* 

# Compare catch curves between cohorts with an LM # 
**Shinnecock Year*Cohort **
```{r}
options(contrasts=c("contr.treatment","contr.poly"))
yearsS <- lm(lncpue~past.peak*Year*co, data=all.co.shin, type=3, na.action=na.omit)
car::Anova(yearsS)
#extract the parameter estimates and confidence intervals
yearsS.tab <- cbind(coef=coef(yearsS),confint(yearsS))
yearsS.tab 

#Tukey Test
df<-df.residual(yearsS)
MSerror<-deviance(yearsS)/df
comparison <- HSD.test(yearsS,c("Year","co"),alpha= 0.05/6, MSerror=MSerror,  group=TRUE)
comparison
```
According to this LM no difference between years*cohorts in terms of slope.
mildly significant difference between intercepts. but this doesn't come out in the tukey test for some reason. 

**The LS means way**
```{r}
yearsS <- lm(lncpue~past.peak*Year/co, data=all.co.shin, type=3, na.action=na.omit)
anova(yearsS)
#coefficients
yearsS$coefficients
#trends
yrs.list <-lstrends(yearsS,c("Year","co"), var="past.peak")
#compare slopes
bonf.alpha <-0.05/6
bonf.alpha
p <- as.data.frame(pairs(yrs.list)) %>% mutate(sigp = ifelse(p.value <= bonf.alpha,"sig","not sig"))
p
```
ONE significant differences between cohorts, but no reason to compare them... 

Try creating the cohort year.
This gives you a different result in the LS means. HOWEVER i think the other way where cohort is nested within year is more appropriate. 

**The LS means way**
```{r}
all.co.shin <-unite(all.co.shin, YearCo, Year,co,remove=FALSE)
yearsS <- lm(lncpue~past.peak*YearCo, data=all.co.shin, type=3, na.action=na.omit)
anova(yearsS)
#coefficients
yearsS$coefficients
#trends
yrs.list <-lstrends(yearsS,c("YearCo"), var="past.peak")
#compare slopes
p <- as.data.frame(pairs(yrs.list))
bonf.alpha <-0.05/6
bonf.alpha
p <- p %>% mutate(sigp = ifelse(p.value <= bonf.alpha,"sig","not sig"))
p
```
 No significant pairwise differences. 
 
 **HeatMap 4 Anne**
 There is really no need to print this heatmap.
```{r}
cols <- c("(0,0.008]"="#034e7b", "(0.008,0.01]" = "#045a8d", "(0.01,0.05]" = "#2b8cbe", "(0.05,0.1]" = "#74a9cf", "(0.1,0.5]"  = "#a6bddb", "(0.5,1]"="#d0d1e6")
p <- separate(p, contrast, c("bayyear1", "bayyear2"),sep="-")
p<-mutate(p, p_if_sig=ifelse(sigp=="sig",p.value,NA), starsig=ifelse(sigp=="sig","*",NA)) 
p<-mutate(p, p_value = cut(p.value, breaks=c(0,0.008, 0.01,0.05,0.1,0.5,1)))
ggplot(p, aes(bayyear1,bayyear2, label=starsig))+
  geom_tile(aes(fill=p_value), show.legend = TRUE)+
  scale_fill_manual(values=cols)+
  geom_text(color="white", size=3)+
  xlab("")+ylab("")+
  theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "black"))
 
```

#Extract cohort catch curve estimates#
```{r, include=FALSE}

#con <-as.data.frame(confint(cc_reg))
#ccN_10 <-bind_cols(ccN_10,con)

#Shi17_e
ce_reg <-catchCurve(cpue~past.peak,data=shi17_e, ages2use=0:35, na.action=na.omit)
summary(ce_reg, verbose=TRUE)
con <-as.data.frame(confint(ce_reg))
ceSh_17 <-cbind(con,as.data.frame(summary(ce_reg)))
ceSh_17e <-tibble::rownames_to_column(ceSh_17,"estimator") %>%mutate(Bay="Shinnecock", Year ="2017", co="early")

#Shi17_l
ce_reg <-catchCurve(cpue~past.peak,data=shi17_l, ages2use=0:63,na.action=na.omit)
summary(ce_reg, verbose=TRUE)
con <-as.data.frame(confint(ce_reg))
ceSh_17 <-cbind(con,as.data.frame(summary(ce_reg)))
ceSh_17l <-tibble::rownames_to_column(ceSh_17,"estimator") %>%mutate(Bay="Shinnecock", Year ="2017", co="late")

#Shi16_e
ce_reg <-catchCurve(cpue~past.peak,data=shi16_e, ages2use=0:64, na.action=na.omit)
summary(ce_reg, verbose=TRUE)
con <-as.data.frame(confint(ce_reg))
ceSh_16 <-cbind(con,as.data.frame(summary(ce_reg)))
ceSh_16e <-tibble::rownames_to_column(ceSh_16,"estimator") %>%mutate(Bay="Shinnecock", Year ="2016", co="early")

#Shi16_l
ce_reg <-catchCurve(cpue~past.peak,data=shi16_l, ages2use=0:43, na.action=na.omit)
summary(ce_reg, verbose=TRUE)
con <-as.data.frame(confint(ce_reg))
ceSh_16 <-cbind(con,as.data.frame(summary(ce_reg)))
ceSh_16l <-tibble::rownames_to_column(ceSh_16,"estimator") %>%mutate(Bay="Shinnecock", Year ="2016", co="late")


cohort_cc <-bind_rows(ceSh_17e,ceSh_17l,ceSh_16e,ceSh_16l)
ccZyr<-filter(cohort_cc, estimator=="Z") %>% group_by(Year) %>% arrange(co)
ccZ<-filter(cohort_cc, estimator=="Z") %>% group_by(co) %>% arrange(Year)
ccA<-filter(cohort_cc, estimator=="A") %>% group_by(Year) %>% arrange(co)
```

**Plot estimates of Z**
```{r}
#sunnycolors <-c("#006600","#ec4847","#6600ff", "#e6df44", "#061283", "#5bd0c8")
ccZ <-as.data.frame(ccZ)%>% unite(YearCo, Year, co, remove=FALSE)
ccZ %>%
  ggplot(aes(x=YearCo, y=Estimate, fill=co)) +
  geom_bar(stat="identity",alpha=1)+
  geom_errorbar(aes(ymin=`95% LCI`, ymax=`95% UCI`),
                  width=.2,position=position_dodge(.9), colour="lightgrey")+
  #scale_fill_manual(values=sunnycolors)+
  scale_fill_grey()+
  xlab("")+ylab("est. Z")+
  theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
#ggsave("comortalitybarplot.png", path="/Users/tdolan/Documents/Proposal/Figures")
#dev.off()
```

```{r}
knitr::kable(ccZ, caption="Instantaneous Mortality (Z)")
knitr::kable(ccZyr, caption="Instantaneous Mortality (Z)")
knitr::kable(ccA, caption="Annual Mortality (A)")
```
Nice, so what we've found from this comparison, is that in shinnecock bay at least, there is no difference between years and cohorts in terms of mortality. This really suggests that Shinnecock is pretty consistent and also that the drivers of mortality are relatively consistent. I think shinnecock could be one of those source populations and the other bays like insurance populations. See Secor storage effect. this could be a discussion point. 

**Mattituck Catch Curves**
```{r, include=FALSE}
#mt16_e
ce_reg <-catchCurve(cpue~past.peak,data=mt16_e, ages2use=0:67)
summary(ce_reg, verbose=TRUE)
con <-confint(ce_reg)
ce <-as.data.frame(cbind(summary(ce_reg),con))
ceMt_16e <-tibble::rownames_to_column(ce,"estimator") %>%mutate(Bay="Mattituck", Year ="2016", co="early")
#mt16_l
ce_reg <-catchCurve(cpue~past.peak,data=mt16_l, ages2use=0:43)
summary(ce_reg, verbose=TRUE)
con <-confint(ce_reg)
ce <-as.data.frame(cbind(summary(ce_reg),con))
ceMt_16l <-tibble::rownames_to_column(ce,"estimator") %>%mutate(Bay="Mattituck", Year ="2016", co="late")

#extract values
mt_slopes <-bind_rows(ceMt_16e,ceMt_16l)
```

```{r}
knitr::kable(mt_slopes, caption="Results of Mattituck cohort Catch Curve")
```

Plot estimates of z with mattituck
**Plot estimates of Z**
```{r}
cohortcolors <-c("#a6bddb","#1c9099")
mt_slopes <-as.data.frame(mt_slopes)%>% unite(YearCo, Year, co, remove=FALSE)
mtZ <-filter(mt_slopes, estimator=="Z")
mt_slopes2 <-bind_rows(mtZ, ccZ)

mt_slopes2 <- unite(mt_slopes2, BayYear, Bay, Year, sep=" ", remove=FALSE)

mt_slopes2 %>%
 #filter(Bay=="Mattituck")%>%
  ggplot(aes(x=YearCo, y=Estimate, fill=co)) +
  geom_bar(stat="identity",alpha=1)+
  geom_errorbar(aes(ymin=`95% LCI`, ymax=`95% UCI`),
                  width=.2,position=position_dodge(.9), colour="lightgrey")+
  scale_fill_manual(values=cohortcolors)+
  xlab("")+ylab("est. Z")+
  facet_wrap(~BayYear, scales="free_x")+
  theme(legend.position="none",axis.title.y = element_text(size = 10),axis.text.y = element_text(size = 10, angle = 0, hjust = 1, vjust = 0, face = "plain"), axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
  
#ggsave("comortalitybarplotNEW.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()
  
```

**Notes**
The mortalities SEEM very different but are not significantly different. probably a degrees of freedom issue? 
We are supposed to use weighted catch curves (Ogle p. 218) but I can't get them to work. 
Residuals are a way of getting at year class strength, but since our ages represent weeks and not year classes, idk why that would be useful here. 
The coefficients in the plots are not exactly the coefficients in the catch curve regressions  (weird) so maybe best not to include the slopes in the plots. 

**LM**
```{r}
yearsS <- lm(lncpue~past.peak*co, data=Mtcocatch, type=3,na.action=na.omit)
car::Anova(yearsS)
#extract the parameter estimates and confidence intervals
yearsS.tab <- cbind(coef=coef(yearsS),confint(yearsS))
yearsS.tab 

#Tukey Test
df<-df.residual(yearsS)
MSerror<-deviance(yearsS)/df
comparison <- HSD.test(yearsS,c("co"),MSerror=MSerror,  group=TRUE)
comparison
```
there do seem to be signficant differences for cohort in mattituck. 


**The LS means way**
```{r}
yearsS <- lm(lncpue~past.peak*co, data=Mtcocatch, type=3,na.action=na.omit)
anova(yearsS)
#coefficients
yearsS$coefficients
#trends
yrs.list <-lstrends(yearsS,c("co"), var="past.peak")
#compare slopes
p <- as.data.frame(pairs(yrs.list))
bonf.alpha <-0.05
bonf.alpha
p <- p %>% mutate(sigp = ifelse(p.value <= bonf.alpha,"sig","not sig"))
p
```


**GROWTH** **GROWTH** **GROWTH** **GROWTH** 

## Growth, as average length ##
I think it's best to actually report this as a table instead of as a series of plots
Growth in each bay in each year, split by cohorts if that applies. 
```{r, fig.align='center', echo=FALSE}
allmelt <-separate(allmelt,Date, c("year","month","d"), remove=FALSE) %>%unite(col="Day",month,d, sep="-", remove=TRUE)%>% unite(BayYear, Bay, Year, remove=FALSE)
####line to remove the outlier point from Jamaica####
allmelt <-mutate(allmelt, value=ifelse(BayYear=="Jamaica_2010" & value==41,NA,value))

```


```{r, fig.align='center', echo=FALSE}

shinco <-all.sco %>%dplyr::select(-X) %>% separate(Date, c("year","m","d"), remove=FALSE) %>%unite(col="Day",m,d, sep="-")%>%
  mutate(numDate = as.numeric(as.POSIXct(Date)),co=as.factor(co)) %>% 
  mutate(star=ifelse(Year==2016 & Week %in% c(5,6,7,8,9),"star",NA)) %>% mutate(star=ifelse(is.na(star)& Year=="2017" & Week %in% c(1,2,3,4,5,6),"star",star))

#Mattituck
Mtco2016 <-separate(Mtco2016,Date, c("year","m","d"), remove=FALSE) %>%unite(col="Day",m,d, sep="-") %>%mutate(co=as.factor(co)) %>%mutate(star=ifelse(Week %in% c(9, 11, 13, 15),"star",NA)) 

coco <-bind_rows(shinco, Mtco2016)%>%unite(BayYear, Bay, Year, sep=" ")

#shinnecock - remember to turn star on or off. 
coco %>%
  #filter(star=="star")%>%
ggplot(aes(y=value,x=Day))+
  geom_point(aes(y=value,x=Day,fill=co,color=co),shape=20)+
  #scale_x_discrete()+
   scale_x_discrete(breaks=c("06-02","07-11","08-08","06-01","07-01","08-04","06-02","07-05","08-02"),
                   labels=c("June","July","August","June","July","August","June","July","August"))+
  geom_smooth(aes(group=co,y=value,x=Day,fill=co, color=co), method="lm", formula= my.formula, se=TRUE, fullrange=FALSE, size=0.5)+
  scale_fill_manual(values=cohortcolors)+scale_color_manual(values=cohortcolors)+
  #stat_poly_eq(formula = my.formula, aes(group=co,y=value,x=Day,label = paste(..eq.label.., ..rr.label.., sep = "~~~")), parse = TRUE) +     
  ylab("length (mm)")+
  facet_grid(~BayYear,scales="free_x")+
  theme_cowplot()
  #theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
#ggsave("growthslopecoALL_notstarredMONTH.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()

```
Based on how these look there are not differences in shinnecock, but maybe differences in mattituck? how do we tell, except just like an anova. 

**the lagged analysis**
```{r}
splitco <-shinco%>%
  split(list(.$Year,.$co)) 

#  function to set the days since settlement
setday <- function(df){
select.day <-c()
pp <-filter(df, !is.na(settled))
for (i in 2:length(pp$Date)){
  select.day[i] <-pp$Date[i]-pp$Date[1]}
select.day[1] <-0
pp <-cbind(pp, select.day)
df <-full_join(df,pp)
}
#settlement dates were visually determined as the start of the appearance of that cohort. 
s16e <- mutate(splitco$`2016.1`, settled=ifelse(Week >= 1, Date,NA)) %>% setday()
s16l <- mutate(splitco$`2016.2`, settled=ifelse(Week >= 5, Date,NA)) %>% setday()
s17e <- mutate(splitco$`2017.1`, settled=ifelse(Week >= 1, Date,NA)) %>% setday()
s17l <- mutate(splitco$`2017.2`, settled=ifelse(Week >= 2, Date,NA)) %>% setday()


splitco <-Mtco2016%>%
  split(list(.$co)) 
mt16e <- mutate(splitco$`1`, settled=ifelse(Week >= 3, Date,NA)) %>% setday()
mt16l <- mutate(splitco$`2`, settled=ifelse(Week >= 7, Date,NA)) %>% setday()

coco2 <-bind_rows(s16e,s16l,s17e,s17l,mt16e,mt16l)%>%unite(BayYear, Bay, Year, sep=" ")

#remember to turn star on or off. 
coco2 %>%
  #filter(star=="star")%>%
ggplot(aes(y=value,x=select.day))+
  geom_point(aes(y=value,x=select.day, color=co), shape=20)+
  #scale_x_discrete()+
  geom_smooth(aes(group=co,y=value,x=select.day,fill=co, color=co), method="lm", formula= my.formula, se=TRUE, fullrange=FALSE, size=0.5)+
  scale_fill_manual(values=cohortcolors)+scale_color_manual(values=cohortcolors)+
  ylab("length (mm)")+ ggtitle("Shinnecock cohorts")+ xlab("Days since selection to the gear")+
  facet_grid(~BayYear,scales="free_x")+
  #theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
  theme_cowplot()
#ggsave("growthslopecoSH_laggedMONTH.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()

```



# Cohort viridis # This doesn't work. 
```{r,fig.align='center', echo=FALSE, eval=FALSE}
### Make the viridis plot for all the bays. 
library("ggridges")
library("forcats")
cohortcolors <-c("#a6bddb","#1c9099")

#Shinnecock 2016#
shinco %>%
  filter(Year==2016, value <= 120)%>% 
  mutate(Day = fct_rev(as.factor(Day))) %>%
  ggplot(aes(y=Day)) +
  stat_density_ridges(aes(x=value,y=Day, fill=co),
                      alpha = 0.8, color="white", from=0, to=130,quantile_lines = TRUE, quantiles = c(0.025, 0.975))+
  scale_discrete_manual("point_color", values = cohortcolors)+
  labs(x = "length (mm)",
       y = "Week",
       #title = "YOY Winter Flounder by length",
       subtitle = "Shinnecock 2016",
       caption = "") +
  scale_y_discrete(expand = c(0.01, 0)) +
  scale_x_continuous(expand = c(0.01, 0)) +
  scale_fill_cyclical(breaks = c("1","2"),
                     labels = c(`1`="early",`2` = "late"),
                     values = c("#a6bddb","#1c9099"),
                      name = "cohort", guide = "legend") +
  theme_ridges(grid = FALSE)+
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "transparent",colour = NA),
    plot.background = element_rect(fill = "transparent",colour = NA)
  )
#ggsave("cohortvir_shi16.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")

#Shinnecock 2017#
shinco %>%
  filter(Year==2017, value <= 120)%>% 
  mutate(Day = fct_rev(as.factor(Day))) %>%
  ggplot(aes(y=Day)) +
  stat_density_ridges(aes(x=value,y=Day, fill=co),
                      alpha = 0.8, color="white", from=0, to=130,quantile_lines = TRUE, quantiles = c(0.025, 0.975))+
  scale_discrete_manual("point_color", values = cohortcolors)+
  labs(x = "length (mm)",
       y = "Week",
       #title = "YOY Winter Flounder by length",
       subtitle = "Shinnecock 2017",
       caption = "") +
  scale_y_discrete(expand = c(0.01, 0)) +
  scale_x_continuous(expand = c(0.01, 0)) +
  scale_fill_cyclical(breaks = c("1","2"),
                     labels = c(`1`="early",`2` = "late"),
                     values = c("#a6bddb","#1c9099"),
                      name = "cohort", guide = "legend") +
  theme_ridges(grid = FALSE)+
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "transparent",colour = NA),
    plot.background = element_rect(fill = "transparent",colour = NA)
  )
#ggsave("cohortvir_shi17.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")

#Mattituck 2016#
Mtco2016 %>%
  filter(Year==2016, value <= 120)%>% 
  mutate(Week = fct_rev(as.factor(Week))) %>%
  ggplot(aes(y=Week)) +
  stat_density_ridges(aes(x=value,y=Week, fill=co),
                      alpha = 0.8, color="white", from=0, to=130,quantile_lines = TRUE, quantiles = c(0.025, 0.975))+
  scale_discrete_manual("point_color", values = cohortcolors)+
  labs(x = "length (mm)",
       y = "Week",
       #title = "YOY Winter Flounder by length",
       subtitle = "Mattituck 2016",
       caption = "") +
  scale_y_discrete(expand = c(0.01, 0)) +
  scale_x_continuous(expand = c(0.01, 0)) +
  scale_fill_cyclical(breaks = c("1","2"),
                     labels = c(`1`="early",`2` = "late"),
                     values = c("#a6bddb","#1c9099"),
                      name = "cohort", guide = "legend") +
  theme_ridges(grid = FALSE)+
  theme(
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(),
    panel.background = element_rect(fill = "transparent",colour = NA),
    plot.background = element_rect(fill = "transparent",colour = NA)
  )
#ggsave("cohortvir_mt16.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()
```

# Growth comparisons #
**ALL growth, starred and non-starred**
**NOT LAGGED**
**Shinnecock Year*Cohort **
```{r}
yearsS <- lm(value~Date*Year*co, data=shinco, type=3, na.action=na.omit)
car::Anova(yearsS)
#extract the parameter estimates and confidence intervals
yearsS.tab <- cbind(coef=coef(yearsS),confint(yearsS))
yearsS.tab 

#Tukey Test
df<-df.residual(yearsS)
MSerror<-deviance(yearsS)/df
comparison <- HSD.test(yearsS,c("Year", "co"),alpha=0.05/28,MSerror=MSerror,  group=TRUE)
comparison
```
**slopes:** year and co interactions are significant. so we should find a way to compare slopes
**Intercepts:** cohort 1 is different between years, but cohort 2 is not! 

# How to tell if slopes are significantly different ###? 
**lsmeans way**
This has to be numDate for some reason, Date doesn't work. 
```{r}
library("lsmeans")
by <-lm(value~numDate*Year*co, data=shinco, type=3, na.action=na.omit)
anova(by)
#coefficients
by$coefficients
#trends
by.list <-lstrends(by,c("Year","co"), var="numDate")
#compare slopes
bonf.alpha <-0.05/6
bonf.alpha
p <- as.data.frame(pairs(by.list)) %>% mutate(sigp = ifelse(p.value <= bonf.alpha,"sig","not sig"))
p
```

**Heatmap 4 Anne**
Not really a reason to print this heatmap either. 
```{r}
cols <- c("(0,0.008]"="#034e7b", "(0.008,0.01]" = "#045a8d", "(0.01,0.05]" = "#2b8cbe", "(0.05,0.1]" = "#74a9cf", "(0.1,0.5]"  = "#a6bddb", "(0.5,1]"="#d0d1e6")
p <- separate(p, contrast, c("bayyear1", "bayyear2"),sep="-")
p<-mutate(p, p_if_sig=ifelse(sigp=="sig",p.value,NA), starsig=ifelse(sigp=="sig","*",NA)) 
p<-mutate(p, p_value = cut(p.value, breaks=c(0,0.008, 0.01,0.05,0.1,0.5,1)))
ggplot(p, aes(bayyear1,bayyear2, label=starsig))+
  geom_tile(aes(fill=p_value), show.legend = TRUE)+
  scale_fill_manual(values=cols)+
  geom_text(color="white", size=3)+
  xlab("")+ylab("")+
  theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "black"))
```



**Mattituck growth anova** We are only comparing 2016
NOT LAGGED AND NOT STARRED.
```{r}
yearsM <- lm(value~Date*co, data=Mtco2016, type=3, na.action=na.omit)
car::Anova(yearsM)

#Tukey Test
df<-df.residual(yearsM)
MSerror<-deviance(yearsM)/df
comparison <- HSD.test(yearsS,c("co"),MSerror=MSerror,  group=TRUE)
comparison

em16 <-lm(value~Date, data=filter(Mtco2016,co==1), type=3, na.action=na.omit)
lm16 <-lm(value~Date, data=filter(Mtco2016,co==2), type=3, na.action=na.omit)

summary(em16)$coefficients[2] 
summary(lm16)$coefficients[2]
```



**Extract regression info from growth cohort slopes**
```{r}

#Shinnecock.
shinco <-unite(shinco, YearCo, Year, co, remove=FALSE)
models <-shinco%>% base::split(.$YearCo)%>%
 purrr::map(function(df) lm(value~Date,data = df,na.action=na.omit))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
CO_ests<-as.data.frame(tibble::rownames_to_column(ests, var ="YearCo")) 

#Mattituck.
models <-Mtco2016%>% base::split(.$co)%>%
 purrr::map(function(df) lm(value~Date,data = df,na.action=na.omit))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
MTCO_ests<-as.data.frame(tibble::rownames_to_column(ests, var ="co"))

```



Different slopes, different intercepts. cohort 1 again has higher growth.... i need to investigate this. 



**Cohort growth estimates barplot**
```{r}

CO_ests <-mutate(CO_ests, Bay="Shinnecock") %>% separate(YearCo, c("Year", "co"), remove=FALSE)
MTCO_ests <-mutate(MTCO_ests, Bay="Mattituck", Year="2016") %>% unite(YearCo, Year, co, sep="_", remove=FALSE)

coco_ests <-bind_rows(CO_ests,MTCO_ests) %>% unite(BayYear, Bay, Year, sep=" ", remove=FALSE)

coco_ests %>%
  ggplot(aes(x=YearCo, y=slope, fill=co)) +
  geom_bar(stat="identity",alpha=1)+
  geom_errorbar(aes(ymin=LCI.slope, ymax=UCI.slope),
                  width=.2,position=position_dodge(.9), colour="lightgrey")+
  scale_fill_manual(values=cohortcolors)+
  xlab("")+ylab("estimated G")+
  facet_grid(~BayYear, scales="free_x")+
  theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
#ggsave("COHORTgrowthbarplot.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()
```

The first cohort grows faster because it had a head start. This is not lagged data. Now we need to repeat the analysis with lagged data. 

# Growth comparisons #
**ALL growth, starred and non-starred**
**LAGGED**
**Shinnecock Year*Cohort **
```{r}
Shincoco <-filter(coco2, BayYear !="Mattituck 2016") %>% separate(BayYear, c("Bay","Year"),remove=FALSE)


yearsS <- lm(value~select.day*Year*co, data=Shincoco, type=3, na.action=na.omit)
car::Anova(yearsS)
#extract the parameter estimates and confidence intervals
yearsS.tab <- cbind(coef=coef(yearsS),confint(yearsS))
yearsS.tab 

#Tukey Test
df<-df.residual(yearsS)
MSerror<-deviance(yearsS)/df
comparison <- HSD.test(yearsS,c("Year", "co"),alpha=0.05/28,MSerror=MSerror,  group=TRUE)
comparison
```

LAGGED lsmeans
```{r}
library("lsmeans")
by <-lm(value~select.day*Year*co, data=Shincoco, type=3, na.action=na.omit)
anova(by)
#coefficients
by$coefficients
#trends
by.list <-lstrends(by,c("Year","co"), var="select.day")
#compare slopes
bonf.alpha <-0.05/6
bonf.alpha
p <- as.data.frame(pairs(by.list)) %>% mutate(sigp = ifelse(p.value <= bonf.alpha,"sig","not sig"))
p
```

**Mattituck growth anova** We are only comparing 2016
LAGGED NOT STARRED.
```{r}
Mtcoco <-filter(coco2, BayYear =="Mattituck 2016") %>% separate(BayYear, c("Bay","Year"),remove=FALSE)

yearsM <- lm(value~select.day*co, data=Mtcoco, type=3, na.action=na.omit)
car::Anova(yearsM)

#Tukey Test
df<-df.residual(yearsM)
MSerror<-deviance(yearsM)/df
comparison <- HSD.test(yearsS,c("co"),MSerror=MSerror,  group=TRUE)
comparison
```


LAGGED not starred. 
```{r}
#Shinnecock.
Shincoco <-unite(Shincoco, YearCo, Year, co, remove=FALSE)
models <-Shincoco%>% base::split(.$YearCo)%>%
 purrr::map(function(df) lm(value~select.day,data = df,na.action=na.omit))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
CO_ests<-as.data.frame(tibble::rownames_to_column(ests, var ="YearCo")) 

#Mattituck.
models <-Mtcoco%>% base::split(.$co)%>%
 purrr::map(function(df) lm(value~select.day,data = df,na.action=na.omit))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
MTCO_ests<-as.data.frame(tibble::rownames_to_column(ests, var ="co"))

```

LAGGED estimates. 
```{r}

CO_ests <-mutate(CO_ests, Bay="Shinnecock") %>% separate(YearCo, c("Year", "co"), remove=FALSE)
MTCO_ests <-mutate(MTCO_ests, Bay="Mattituck", Year="2016") %>% unite(YearCo, Year, co, sep="_", remove=FALSE)

coco_ests <-bind_rows(CO_ests,MTCO_ests) %>% unite(BayYear, Bay, Year, sep=" ", remove=FALSE)

coco_ests %>%
  ggplot(aes(x=YearCo, y=slope, fill=co)) +
  geom_bar(stat="identity",alpha=1)+
  geom_errorbar(aes(ymin=LCI.slope, ymax=UCI.slope),
                  width=.2,position=position_dodge(.9), colour="lightgrey")+
  scale_fill_manual(values=cohortcolors)+
  xlab("")+ylab("estimated G")+
  facet_grid(~BayYear, scales="free_x")+
  theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
#ggsave("COHORTgrowthbarplotLAGGED.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()
```

NOW we just have to make the STARRED version. 
**STARRED not lagged ***
```{r}
Starshin <-filter(Shincoco, !is.na(star))

yearsS <- lm(value~Date*Year*co, data=Starshin, type=3, na.action=na.omit)
car::Anova(yearsS)
#extract the parameter estimates and confidence intervals
yearsS.tab <- cbind(coef=coef(yearsS),confint(yearsS))
yearsS.tab 

#Tukey Test
df<-df.residual(yearsS)
MSerror<-deviance(yearsS)/df
comparison <- HSD.test(yearsS,c("Year", "co"),alpha=0.05/28,MSerror=MSerror,  group=TRUE)
comparison
```

STARRED lsmeans
```{r}
library("lsmeans")
by <-lm(value~numDate*Year*co, data=Starshin, type=3, na.action=na.omit)
anova(by)
#coefficients
by$coefficients
#trends
by.list <-lstrends(by,c("Year","co"), var="numDate")
#compare slopes
bonf.alpha <-0.05/6
bonf.alpha
p <- as.data.frame(pairs(by.list)) %>% mutate(sigp = ifelse(p.value <= bonf.alpha,"sig","not sig"))
p
```




**Mattituck growth anova** We are only comparing 2016
STARRED.
```{r}
Mtstar <-filter(Mtcoco, !is.na(star))

yearsM <- lm(value~Date*co, data=Mtstar, type=3, na.action=na.omit)
car::Anova(yearsM)

#Tukey Test
df<-df.residual(yearsM)
MSerror<-deviance(yearsM)/df
comparison <- HSD.test(yearsS,c("co"),MSerror=MSerror,  group=TRUE)
comparison
```

STARRED
```{r}
#Shinnecock.
models <-Starshin%>% base::split(.$YearCo)%>%
 purrr::map(function(df) lm(value~Date,data = df,na.action=na.omit))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
CO_ests<-as.data.frame(tibble::rownames_to_column(ests, var ="YearCo")) 

#Mattituck.
models <-Mtstar%>% base::split(.$co)%>%
 purrr::map(function(df) lm(value~select.day,data = df,na.action=na.omit))

f <-models %>% map(summary) %>% map_dbl(~.$coefficients[2]) #slope
g <-models %>% map(summary) %>% map_dbl(~.$coefficients[4]) #std error of the slope
h <-models %>% map(summary) %>% map_dbl(~.$coefficients[6]) #t value of the slope
i <-models %>% map(summary) %>% map_dbl(~.$coefficients[8]) #p value of the slope
j <-models %>% map(confint) %>% map_dbl(~.[2]) #LCI of slope
k <-models %>% map(confint) %>% map_dbl(~.[4]) #UCI of slope
a <-models %>% map(summary) %>% map_dbl(~.$adj.r.squared) #r squared
b <-models %>% map(summary) %>% map_dbl(~.$coefficients[1])#y intercept
c <-models %>% map(summary) %>% map_dbl(~.$fstatistic[1]) #f statistic
d <-models %>% map(summary) %>% map_dbl(~.$df[2])
e <-models %>% map_dbl(lmp) #p value

est_list <-list(f,g,h,i,j,k,a,b,c,d,e)
ests <-do.call("rbind",est_list)
rownames(ests) <-c("slope","std.err.slope","t.value.slope","p.val.slope","LCI.slope","UCI.slope","adj.r.sq","y.int","f.stat","dof","p.val")
ests <- as.data.frame(t(ests))
MTCO_ests<-as.data.frame(tibble::rownames_to_column(ests, var ="co"))

```

STARRED estimates. 
```{r}
CO_ests <-mutate(CO_ests, Bay="Shinnecock") %>% separate(YearCo, c("Year", "co"), remove=FALSE)
MTCO_ests <-mutate(MTCO_ests, Bay="Mattituck", Year="2016") %>% unite(YearCo, Year, co, sep="_", remove=FALSE)

coco_ests <-bind_rows(CO_ests,MTCO_ests) %>% unite(BayYear, Bay, Year, sep=" ", remove=FALSE)

coco_ests %>%
  ggplot(aes(x=YearCo, y=slope, fill=co)) +
  geom_bar(stat="identity",alpha=1)+
  geom_errorbar(aes(ymin=LCI.slope, ymax=UCI.slope),
                  width=.2,position=position_dodge(.9), colour="lightgrey")+
  scale_fill_manual(values=cohortcolors)+
  xlab("")+ylab("estimated G")+
  facet_grid(~BayYear, scales="free_x")+
  theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
#ggsave("COHORTgrowthbarplotSTARRED.png", path="/Users/tdolan/Documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()
```


## **COHORT CDFS** ##
The source for these methods is taraCDFS2.Rmd