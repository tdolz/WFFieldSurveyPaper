---
title: "TaraCDF_skyler"
author: "tara"
date: "5/20/2020"
output:
  pdf_document: default
  html_document: default
theme: readable
---

```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, cache=TRUE)
```

## R Markdown

Converting this to markdown because it's easier to keep it together. 

**Analyses**  
+ Are there significant differences between occupied and available habitat *within each bay* in terms of temperature, salinity, dissolved oxygen.  
    - Weight catch by year. test stat is the permutation test. separate analyses for each bay.      
+ Are there differences *between bays* in time [date] and temperature when fish become fully selected to the gear [first peak in abundance].  
    - subset the data leading up to the peak abundance. find the temperature at the 50th percentile. plot all the bays on one graph. weight by bay. could do this separately for shinnecock peaks. 


**Total sampled area**
I did some rough area calculations in google earth based on previous sample maps. 
Sample maps can be found in the powerpoint "Sampling maps w stations for first SK grant 2016"
Shinnecock: 1.61 KM
Moriches: 1.49 KM
Jamaica Bay: 2.24 KM
Napeague Harbor: 1.98 KM
Cold Spring Pond: 0.52 KM  
Mattituck Creek: 0.37 KM

**I think I need to measure these for real somehow to determine which bay is the most productive per unit area**
https://jamesepaterson.github.io/jamespatersonblog/03_trackingworkshop_homeranges
we could do this by calculating minimum convex polygons but we'd have to fix lyndie's data for Cold Spring pond first. 
We should do this in a different R script.


```{r, echo=FALSE, warning=FALSE}
library("tidyr")
library("ggplot2")
library("plyr")
library("purrr")
library("dplyr")
```

**My Data**
```{r}
#keeping everything in this folder
setwd("/Users//tdolan/Documents//R-Github//WFFieldSurveyPaper")
somedata3<-read.csv("skcompiled4gams2.csv", na.strings="", header=TRUE)
#somedata4 <-dplyr::select(somedata3,-rcpT,-std_cpue,-cpue)
```

## Catch per area at the peak ##
**Create the prepeak dataset**
```{r}
eday <- function(df){
  edays <-c()
for (i in 2:length(df$Date)){
  edays[i] <-df$Date[i]-df$Date[1]}
edays[1] <-0
df <-cbind(df,edays)
}
prepeak <-somedata3%>% dplyr::select(-minl, -maxl, -sdl, -distswept, -Tow, -Towindex) %>% mutate(Date=as.Date(Date))%>%
  unite(BayYear,Bay,Year, remove=FALSE)%>% arrange(Date)%>% base::split(.$BayYear) %>% map(eday)  #very important to arrange by date first. 
somedata3 <-mutate(somedata3,Date=as.Date(Date), Year=as.factor(Year), area=!is.na(area))
findpeak <-ddply(somedata3, Bay~Year~Date, summarize, new.cpue=sum(cpT)/sum(area))

#Mattituck
MTtows <-read.csv("mt YOY length and weight.csv",header=TRUE)
MTtows <-mutate(MTtows, len = as.numeric(Length..mm.), Tow=as.factor(Tow), Date=as.Date(Date)) %>%mutate(pres = ifelse(is.na(len),0,1))
MTcpt <-ddply(MTtows, Date~Tow, summarize, cpT=sum(pres)) %>% separate(Date, c("Year","m","d"),sep="-", remove=FALSE) %>% dplyr::rename(X=Tow)

#subset the dataset to before the peak by manually looking at what eday corresponds to the maximum cpue, the day before and after. 
prepeak_CSP10 <-filter(prepeak$`Cold Spring Pond_2010`, edays %in% c(0,7)) 
prepeak_J10 <-filter(prepeak$`Jamaica_2010`, edays %in% c(0,17))
prepeak_J11 <-filter(prepeak$`Jamaica_2011`, edays %in% c(0,28,42))
prepeak_J16 <-filter(prepeak$`Jamaica_2016`, edays %in% c(0,7))
prepeak_M10 <-filter(prepeak$`Moriches_2010`, edays %in% c(8,24,44))
prepeak_M11 <-filter(prepeak$`Moriches_2011`, edays %in% c(23,35,49))  
prepeak_M16 <-filter(prepeak$`Moriches_2016`, edays %in% c(23,30,37)) 
prepeak_N10 <-filter(prepeak$`Napeague_2010`, edays %in% c(0,21))
prepeak_N16 <-filter(prepeak$`Napeague_2016`, edays %in% c(21, 27, 35))
prepeak_Sh10 <-filter(prepeak$`Shinnecock_2010`, edays %in% c(9,27, 43)) #First peak
prepeak_Sh11 <-filter(prepeak$`Shinnecock_2011`, edays %in% c(0,25)) #First peak, second peak is at edays=79
prepeak_Sh16 <-filter(prepeak$`Shinnecock_2016`, edays %in% c(23, 30, 37)) #First peak, second peak, which is slightly larger, is at edays=56
prepeak_Sh17 <-filter(prepeak$`Shinnecock_2017`, edays %in% c(12, 20, 26)) #first peak, second peak, which is smaller, is at edays=47
prepeak_MT15 <-filter(MTcpt, Year=="2015") %>% eday() %>% filter(edays  %in% c(0, 13, 27)) %>% mutate(Year=as.integer(Year),X=as.integer(X), Bay="Mattituck")
prepeak_MT16 <<-filter(MTcpt, Year=="2016") %>% eday() %>% filter(edays  %in% c(11, 28, 39)) %>% mutate(Year=as.integer(Year),X=as.integer(X),Bay="Mattituck")
prepeak <-bind_rows(prepeak_CSP10,prepeak_J10,prepeak_J11,prepeak_J16,prepeak_M10,prepeak_M11,prepeak_M16,prepeak_N10,prepeak_N16,prepeak_Sh10,prepeak_Sh11,prepeak_Sh16,prepeak_Sh17, prepeak_MT15, prepeak_MT16) 


```

# **New version 8/23/2020** #

```{r}

#we don't want the standard deviation, we want the standard error of the mean. 
std <- function(x) sd(x)/sqrt(length(x))

## we want the mean catch per area swept across the peak ##
CPArea <-function(df) {
dat.eq1 <-ddply(df, Date~Year, summarize, area_swept=sum(area), total.catch=sum(cpT))%>% mutate(daily.cpue=total.catch/area_swept) 
df.prod <-ddply(dat.eq1, ~Year, summarize, avProd=mean(daily.cpue), seprod=std(daily.cpue))
df.prod}

#use function00
split.bay <- prepeak %>% base::split(.$Bay) 

a <-CPArea(split.bay$Shinnecock)
a <-mutate(a,Year=as.factor(Year), Bay="Shinnecock")
b <-CPArea(split.bay$Jamaica)
b <-mutate(b,Year=as.factor(Year), Bay="Jamaica") 
c <-CPArea(split.bay$Moriches)
c <-mutate(c,Year=as.factor(Year), Bay="Moriches")
d <-CPArea(split.bay$Napeague)
d<-mutate(d,Year=as.factor(Year), Bay="Napeague")
e <-CPArea(split.bay$`Cold Spring Pond`)
e <-mutate(e,Year=as.factor(Year), Bay="Cold Spring Pond")


###Mattituck###
# we do not have per tow area swept for mattituck readily available, so here is a workaround.
MTarea <-read.csv("some_mt_data.csv",header=TRUE) #per week area
MTarea <-dplyr::select(MTarea,Date,area)%>% mutate(Date=as.Date(Date))
prepeak_MT <-bind_rows(prepeak_MT15, prepeak_MT16) #per tow dataset

dat.eq1 <-ddply(prepeak_MT, Date~Year, summarize, total.catch=sum(cpT)) #now we have weekly catch
dat.eq1 <-left_join(dat.eq1, MTarea, by="Date") %>% mutate(daily.cpue=total.catch/area) %>%dplyr::select(Year,daily.cpue)
f <-ddply(dat.eq1, ~Year, summarize, avProd=mean(daily.cpue), seprod=std(daily.cpue))
f <-mutate(f,Year=as.factor(Year), Bay="Mattituck")

peakdens <-bind_rows(a,b,c,d,e,f)%>%unite(BayYear, Bay, Year, sep=" ", remove=FALSE)%>%arrange(Year)
peakdens <-mutate(peakdens, UCI=avProd + seprod, LCI=avProd-seprod)

drabcolors <-c("#f6eff7","#d0d1e6","#a6bddb", "#67a9cf", "#1c9099", "#016450")

peakdens %>%
  filter(Year %in% c(2010,2011,2015,2016,2017))%>%
  ggplot(aes(x=BayYear,y=avProd, fill=Bay)) +
  geom_bar(stat="identity", position="dodge")+ #ylim(c(0,35000))+
  geom_errorbar(aes(ymin=LCI, ymax=UCI),
              width=.2,position=position_dodge(.9), colour="lightgrey")+
  scale_fill_manual(values=drabcolors)+
  #facet_wrap(~year)+
  xlab("")+ylab("mean CPUE at peak abundance")+
  theme(axis.text.x = element_text(angle = 90),panel.background = element_rect(fill = "white", colour = "white"))
#ggsave("bayprod_prepeak.png", path="/Users/tdolan/documents/WF SK PROJ/Survey data/Field Survey Paper/final figures")
#dev.off()


```
















